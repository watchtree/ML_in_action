{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优点：在数据较少的情况下依然有效，可以处理多类别问题\n",
    "\n",
    "缺点：对于输入数据的准备方式较为敏感\n",
    "\n",
    "适用数据类型：标称性数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本概念与认识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**买瓜问题** 介绍先验概率和后验概率\n",
    "\n",
    "最近天气炎热，红色石头来到超市准备买个西瓜，可是没有太多的经验，不知道怎么样才能挑个熟瓜。这时候，作为理科生，红色石头就有这样的考虑：\n",
    "\n",
    "如果我对这个西瓜没有任何了解，包括瓜的颜色、形状、瓜蒂是否脱落。按常理来说，西瓜成熟的概率大概是 60%。那么，这个概率 P(瓜熟) 就被称为**先验概率**。\n",
    "\n",
    "也就是说，先验概率是**根据以往经验和分析得到的概率，先验概率无需样本数据，不受任何条件的影响**。就像红色石头只根据常识而不根据西瓜状态来判断西瓜是否成熟，这就是先验概率。\n",
    "\n",
    "再来看，红色石头以前学到了一个判断西瓜是否成熟的常识，就是看瓜蒂是否脱落。一般来说，瓜蒂脱落的情况下，西瓜成熟的概率大一些，大概是 75%。如果把瓜蒂脱落当作一种结果，然后去推测西瓜成熟的概率，这个概率 P(瓜熟 | 瓜蒂脱落) 就被称为**后验概率**。后验概率**类似于条件概率**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯进行文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词向量转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#创建实验样本，\n",
    "def loadDataSet():\n",
    "    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0, 1, 0, 1, 0, 1]    #1 代表侮辱性的词汇，0不是，对应上面的语句列表\n",
    "    return postingList, classVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**set：** \n",
    "\n",
    "python的set和其他语言类似, 是一个无序不重复元素集, 基本功能包括关系测试和消除重复元素. 集合对象还支持union(联合), intersection(交), difference(差)和sysmmetric difference(对称差集)等数学运算.\n",
    "\n",
    "sets 支持 x in set, len(set),和 for x in set。作为一个无序的集合，sets不记录元素位置或者插入点。因此，sets不支持 indexing, slicing, 或其它类序列（sequence-like）的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建一个包含在文档中中不存在重复的词的列表\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])  #create empty set\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) # 两个集合的并集\n",
    "    #vocabSet为一个不重复此表\n",
    "    return list(vocabSet) #返回转型为list\n",
    "\n",
    "#word2vec输入参数为词汇表，和一个文档\n",
    "#返回的文档向量，对应词汇表中的单词在输入文档中是否出现，创建一个相对应的向量\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0 for i in range(len(vocabList))]\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else: \n",
    "            print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
       "  ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
       "  ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
       "  ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
       "  ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
       "  ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']],\n",
       " [0, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOPosts, ListClasses = loadDataSet()\n",
    "listOPosts, ListClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['has',\n",
       " 'love',\n",
       " 'dalmation',\n",
       " 'so',\n",
       " 'him',\n",
       " 'I',\n",
       " 'help',\n",
       " 'take',\n",
       " 'food',\n",
       " 'ate',\n",
       " 'park',\n",
       " 'my',\n",
       " 'worthless',\n",
       " 'dog',\n",
       " 'stop',\n",
       " 'posting',\n",
       " 'mr',\n",
       " 'not',\n",
       " 'how',\n",
       " 'stupid',\n",
       " 'quit',\n",
       " 'to',\n",
       " 'please',\n",
       " 'maybe',\n",
       " 'cute',\n",
       " 'steak',\n",
       " 'licks',\n",
       " 'is',\n",
       " 'problems',\n",
       " 'buying',\n",
       " 'flea',\n",
       " 'garbage']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myVocabList = createVocabList(listOPosts)\n",
    "myVocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setOfWords2Vec(myVocabList, listOPosts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练并计算概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix) #训练数据数量\n",
    "    numWords = len(trainMatrix[0]) #数据词向量长度\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs) #正类别数量/样本总数\n",
    "    p0Num = np.zeros(numWords); \n",
    "    p1Num = np.zeros(numWords)     #初始化概率计算的分子，由多少个词即有多少个\n",
    "    \n",
    "    p0Denom = 0\n",
    "    p1Denom = 0                        #分母变量初始化，此处加入初值2，作为平滑处理的值防止出现bug\n",
    "    \n",
    "    for i in range(numTrainDocs): #遍历每一个样本呢\n",
    "        if trainCategory[i] == 1: #如果对应分类=1\n",
    "            p1Num += trainMatrix[i] #对应词向量相加为相对应类的和\n",
    "            p1Denom += sum(trainMatrix[i]) #获取正样本总词语数量\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])#获取负样本总词语数量\n",
    "    p1Vect = (p1Num/p1Denom)          #当样本为正样本部分，n为词向量每个出现的概率\n",
    "    p0Vect = (p0Num/p0Denom)          #返回对应样本出现的概率\n",
    "    return p0Vect, p1Vect, pAbusive #返回两个向量，一个概率（正样本概率），二分类可求负样本概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOPosts, listClasses = loadDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
       " ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
       " ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
       " ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
       " ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
       " ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOPosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 1, 0, 1]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ListClasses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['has',\n",
       " 'love',\n",
       " 'dalmation',\n",
       " 'so',\n",
       " 'him',\n",
       " 'I',\n",
       " 'help',\n",
       " 'take',\n",
       " 'food',\n",
       " 'ate',\n",
       " 'park',\n",
       " 'my',\n",
       " 'worthless',\n",
       " 'dog',\n",
       " 'stop',\n",
       " 'posting',\n",
       " 'mr',\n",
       " 'not',\n",
       " 'how',\n",
       " 'stupid',\n",
       " 'quit',\n",
       " 'to',\n",
       " 'please',\n",
       " 'maybe',\n",
       " 'cute',\n",
       " 'steak',\n",
       " 'licks',\n",
       " 'is',\n",
       " 'problems',\n",
       " 'buying',\n",
       " 'flea',\n",
       " 'garbage']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myVocabList = createVocabList(listOPosts)\n",
    "myVocabList "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainMat = []\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainMat)\n",
    "len(trainMat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0V, p1V, pAb = trainNB0(trainMat, listClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.04166667, 0.04166667, 0.04166667, 0.04166667, 0.08333333,\n",
       "        0.04166667, 0.04166667, 0.        , 0.        , 0.04166667,\n",
       "        0.        , 0.125     , 0.        , 0.04166667, 0.04166667,\n",
       "        0.        , 0.04166667, 0.        , 0.04166667, 0.        ,\n",
       "        0.        , 0.04166667, 0.04166667, 0.        , 0.04166667,\n",
       "        0.04166667, 0.04166667, 0.04166667, 0.04166667, 0.        ,\n",
       "        0.04166667, 0.        ]),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.05263158,\n",
       "        0.        , 0.        , 0.05263158, 0.05263158, 0.        ,\n",
       "        0.05263158, 0.        , 0.10526316, 0.10526316, 0.05263158,\n",
       "        0.05263158, 0.        , 0.05263158, 0.        , 0.15789474,\n",
       "        0.05263158, 0.05263158, 0.        , 0.05263158, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05263158,\n",
       "        0.        , 0.05263158]),\n",
       " 0.5)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0V, p1V, pAb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**朴素贝叶斯极大似然估计因为估计的概率值为0的情况，采用贝叶斯分类方法加入平滑处理**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#重载训练器\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix) #训练数据数量\n",
    "    numWords = len(trainMatrix[0]) #数据词向量长度\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs) #正类别数量/样本总数\n",
    "    lamba = 1\n",
    "    p0Num = np.zeros(numWords) + lamba\n",
    "    p1Num = np.zeros(numWords) + lamba #初始化概率计算的分子，由多少个词即有多少个\n",
    "    #加入平滑处理\n",
    "    \n",
    "    p0Denom = lamba*2\n",
    "    p1Denom = lamba*2                     #分母变量初始化，此处加入初值2，作为平滑处理的值防止出现bug\n",
    "    #加入平滑处理\n",
    "        \n",
    "    for i in range(numTrainDocs): #遍历每一个样本呢\n",
    "        if trainCategory[i] == 1: #如果对应分类=1\n",
    "            p1Num += trainMatrix[i] #对应词向量相加为相对应类的和\n",
    "            p1Denom += sum(trainMatrix[i]) #获取正样本总词语数量\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])#获取负样本总词语数量\n",
    "    p1Vect = (p1Num/p1Denom)          #当样本为正样本部分，n为词向量每个出现的概率\n",
    "    p0Vect = (p0Num/p0Denom)          #返回对应样本出现的概率\n",
    "    return p0Vect, p1Vect, pAbusive #返回两个向量，一个概率（正样本概率），二分类可求负样本概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.07692308, 0.07692308, 0.07692308, 0.07692308, 0.11538462,\n",
       "        0.07692308, 0.07692308, 0.03846154, 0.03846154, 0.07692308,\n",
       "        0.03846154, 0.15384615, 0.03846154, 0.07692308, 0.07692308,\n",
       "        0.03846154, 0.07692308, 0.03846154, 0.07692308, 0.03846154,\n",
       "        0.03846154, 0.07692308, 0.07692308, 0.03846154, 0.07692308,\n",
       "        0.07692308, 0.07692308, 0.07692308, 0.07692308, 0.03846154,\n",
       "        0.07692308, 0.03846154]),\n",
       " array([0.04761905, 0.04761905, 0.04761905, 0.04761905, 0.0952381 ,\n",
       "        0.04761905, 0.04761905, 0.0952381 , 0.0952381 , 0.04761905,\n",
       "        0.0952381 , 0.04761905, 0.14285714, 0.14285714, 0.0952381 ,\n",
       "        0.0952381 , 0.04761905, 0.0952381 , 0.04761905, 0.19047619,\n",
       "        0.0952381 , 0.0952381 , 0.04761905, 0.0952381 , 0.04761905,\n",
       "        0.04761905, 0.04761905, 0.04761905, 0.04761905, 0.0952381 ,\n",
       "        0.04761905, 0.0952381 ]),\n",
       " 0.5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0V, p1V, pAb = trainNB0(trainMat, listClasses)\n",
    "p0V, p1V, pAb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**改进过多极小数相乘的向下溢出问题 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#重载训练器\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix) #训练数据数量\n",
    "    numWords = len(trainMatrix[0]) #数据词向量长度\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs) #正类别数量/样本总数\n",
    "    lamba = 1\n",
    "    p0Num = np.zeros(numWords) + lamba\n",
    "    p1Num = np.zeros(numWords) + lamba #初始化概率计算的分子，由多少个词即有多少个\n",
    "    #加入平滑处理\n",
    "    \n",
    "    p0Denom = lamba*2\n",
    "    p1Denom = lamba*2                     #分母变量初始化，此处加入初值2，作为平滑处理的值防止出现bug\n",
    "    #加入平滑处理\n",
    "        \n",
    "    for i in range(numTrainDocs): #遍历每一个样本呢\n",
    "        if trainCategory[i] == 1: #如果对应分类=1\n",
    "            p1Num += trainMatrix[i] #对应词向量相加为相对应类的和\n",
    "            p1Denom += sum(trainMatrix[i]) #获取正样本总词语数量\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])#获取负样本总词语数量\n",
    "    p1Vect = np.log(p1Num/p1Denom)          #当样本为正样本部分，n为词向量每个出现的概率\n",
    "    p0Vect =np.log(p0Num/p0Denom)          #返回对应样本出现的概率\n",
    "    return p0Vect, p1Vect, pAbusive #返回两个向量，一个概率（正样本概率），二分类可求负样本概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-2.56494936, -2.56494936, -2.56494936, -2.56494936, -2.15948425,\n",
       "        -2.56494936, -2.56494936, -3.25809654, -3.25809654, -2.56494936,\n",
       "        -3.25809654, -1.87180218, -3.25809654, -2.56494936, -2.56494936,\n",
       "        -3.25809654, -2.56494936, -3.25809654, -2.56494936, -3.25809654,\n",
       "        -3.25809654, -2.56494936, -2.56494936, -3.25809654, -2.56494936,\n",
       "        -2.56494936, -2.56494936, -2.56494936, -2.56494936, -3.25809654,\n",
       "        -2.56494936, -3.25809654]),\n",
       " array([-3.04452244, -3.04452244, -3.04452244, -3.04452244, -2.35137526,\n",
       "        -3.04452244, -3.04452244, -2.35137526, -2.35137526, -3.04452244,\n",
       "        -2.35137526, -3.04452244, -1.94591015, -1.94591015, -2.35137526,\n",
       "        -2.35137526, -3.04452244, -2.35137526, -3.04452244, -1.65822808,\n",
       "        -2.35137526, -2.35137526, -3.04452244, -2.35137526, -3.04452244,\n",
       "        -3.04452244, -3.04452244, -3.04452244, -3.04452244, -2.35137526,\n",
       "        -3.04452244, -2.35137526]),\n",
       " 0.5)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0V, p1V, pAb = trainNB0(trainMat, listClasses)\n",
    "p0V, p1V, pAb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 朴素贝叶斯分类函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分类函数\n",
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + np.log(pClass1)    #理论上求概率与类别相乘，由于p1vec即为log输出结果，因此与类别相乘改为相加\n",
    "    p0 = sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1)#其实这里时概率相乘，但是因为改成log所以在此时在可以进行相加，条件概率假设独立\n",
    "    #求取在\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOPosts, listClasses = loadDataSet()#生成样例数据\n",
    "myVocabList = createVocabList(listOPosts) #创建词向量列表\n",
    "trainMat = [] #初始化训练矩阵\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc)) \n",
    "#生成训练集矩阵\n",
    "p0V, p1V, pAb = trainNB0(np.array(trainMat), np.array(listClasses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-2.56494936, -2.56494936, -2.56494936, -2.56494936, -2.15948425,\n",
       "        -2.56494936, -2.56494936, -3.25809654, -3.25809654, -2.56494936,\n",
       "        -3.25809654, -1.87180218, -3.25809654, -2.56494936, -2.56494936,\n",
       "        -3.25809654, -2.56494936, -3.25809654, -2.56494936, -3.25809654,\n",
       "        -3.25809654, -2.56494936, -2.56494936, -3.25809654, -2.56494936,\n",
       "        -2.56494936, -2.56494936, -2.56494936, -2.56494936, -3.25809654,\n",
       "        -2.56494936, -3.25809654]),\n",
       " array([-3.04452244, -3.04452244, -3.04452244, -3.04452244, -2.35137526,\n",
       "        -3.04452244, -3.04452244, -2.35137526, -2.35137526, -3.04452244,\n",
       "        -2.35137526, -3.04452244, -1.94591015, -1.94591015, -2.35137526,\n",
       "        -2.35137526, -3.04452244, -2.35137526, -3.04452244, -1.65822808,\n",
       "        -2.35137526, -2.35137526, -3.04452244, -2.35137526, -3.04452244,\n",
       "        -3.04452244, -3.04452244, -3.04452244, -3.04452244, -2.35137526,\n",
       "        -3.04452244, -2.35137526]),\n",
       " 0.5)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0V, p1V, pAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "testEntry = ['love', 'my', 'dalmation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))#生成词向量\n",
    "thisDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifyNB(thisDoc, p0V, p1V, pAb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as:  0\n"
     ]
    }
   ],
   "source": [
    "print(testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "testEntry = ['stupid', 'garbage']\n",
    "thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "print(testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**改进word2vec模型：一个词在一句话中不一定只出现一次**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#词集模型（0-1）转化为词袋模型（0-无穷） （由setOfwords2Vec（）进行改进）\n",
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试\n",
    "listOPosts, listClasses = loadDataSet()#生成样例数据\n",
    "myVocabList = createVocabList(listOPosts) #创建词向量列表\n",
    "trainMat = [] #初始化训练矩阵\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(bagOfWords2VecMN(myVocabList, postinDoc)) \n",
    "#生成训练集矩阵\n",
    "p0V, p1V, pAb = trainNB0(np.array(trainMat), np.array(listClasses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "testEntry = ['stupid', 'garbage']\n",
    "thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "print(testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as:  0\n"
     ]
    }
   ],
   "source": [
    "testEntry = ['love', 'my', 'dalmation']\n",
    "thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "print(testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯过滤垃圾邮件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python 的 **re 模块**（Regular Expression 正则表达式）提供各种正则表达式的匹配操作，在文本解析、复杂字符串分析和信息提取时是一个非常有用的工具，下面我主要总结了re的常用方法\n",
    "\n",
    "1. re的简介\n",
    "    使用python的re模块，尽管不能满足所有复杂的匹配情况，但足够在绝大多数情况下能够有效地实现对复杂字符串的分析并提取出相关信息。python 会将正则表达式转化为字节码，利用 C 语言的匹配引擎进行深度优先的匹配。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support for regular expressions (RE).\n",
      "\n",
      "This module provides regular expression matching operations similar to\n",
      "those found in Perl.  It supports both 8-bit and Unicode strings; both\n",
      "the pattern and the strings being processed can contain null bytes and\n",
      "characters outside the US ASCII range.\n",
      "\n",
      "Regular expressions can contain both special and ordinary characters.\n",
      "Most ordinary characters, like \"A\", \"a\", or \"0\", are the simplest\n",
      "regular expressions; they simply match themselves.  You can\n",
      "concatenate ordinary characters, so last matches the string 'last'.\n",
      "\n",
      "The special characters are:\n",
      "    \".\"      Matches any character except a newline.\n",
      "    \"^\"      Matches the start of the string.\n",
      "    \"$\"      Matches the end of the string or just before the newline at\n",
      "             the end of the string.\n",
      "    \"*\"      Matches 0 or more (greedy) repetitions of the preceding RE.\n",
      "             Greedy means that it will match as many repetitions as possible.\n",
      "    \"+\"      Matches 1 or more (greedy) repetitions of the preceding RE.\n",
      "    \"?\"      Matches 0 or 1 (greedy) of the preceding RE.\n",
      "    *?,+?,?? Non-greedy versions of the previous three special characters.\n",
      "    {m,n}    Matches from m to n repetitions of the preceding RE.\n",
      "    {m,n}?   Non-greedy version of the above.\n",
      "    \"\\\\\"     Either escapes special characters or signals a special sequence.\n",
      "    []       Indicates a set of characters.\n",
      "             A \"^\" as the first character indicates a complementing set.\n",
      "    \"|\"      A|B, creates an RE that will match either A or B.\n",
      "    (...)    Matches the RE inside the parentheses.\n",
      "             The contents can be retrieved or matched later in the string.\n",
      "    (?aiLmsux) Set the A, I, L, M, S, U, or X flag for the RE (see below).\n",
      "    (?:...)  Non-grouping version of regular parentheses.\n",
      "    (?P<name>...) The substring matched by the group is accessible by name.\n",
      "    (?P=name)     Matches the text matched earlier by the group named name.\n",
      "    (?#...)  A comment; ignored.\n",
      "    (?=...)  Matches if ... matches next, but doesn't consume the string.\n",
      "    (?!...)  Matches if ... doesn't match next.\n",
      "    (?<=...) Matches if preceded by ... (must be fixed length).\n",
      "    (?<!...) Matches if not preceded by ... (must be fixed length).\n",
      "    (?(id/name)yes|no) Matches yes pattern if the group with id/name matched,\n",
      "                       the (optional) no pattern otherwise.\n",
      "\n",
      "The special sequences consist of \"\\\\\" and a character from the list\n",
      "below.  If the ordinary character is not on the list, then the\n",
      "resulting RE will match the second character.\n",
      "    \\number  Matches the contents of the group of the same number.\n",
      "    \\A       Matches only at the start of the string.\n",
      "    \\Z       Matches only at the end of the string.\n",
      "    \\b       Matches the empty string, but only at the start or end of a word.\n",
      "    \\B       Matches the empty string, but not at the start or end of a word.\n",
      "    \\d       Matches any decimal digit; equivalent to the set [0-9] in\n",
      "             bytes patterns or string patterns with the ASCII flag.\n",
      "             In string patterns without the ASCII flag, it will match the whole\n",
      "             range of Unicode digits.\n",
      "    \\D       Matches any non-digit character; equivalent to [^\\d].\n",
      "    \\s       Matches any whitespace character; equivalent to [ \\t\\n\\r\\f\\v] in\n",
      "             bytes patterns or string patterns with the ASCII flag.\n",
      "             In string patterns without the ASCII flag, it will match the whole\n",
      "             range of Unicode whitespace characters.\n",
      "    \\S       Matches any non-whitespace character; equivalent to [^\\s].\n",
      "    \\w       Matches any alphanumeric character; equivalent to [a-zA-Z0-9_]\n",
      "             in bytes patterns or string patterns with the ASCII flag.\n",
      "             In string patterns without the ASCII flag, it will match the\n",
      "             range of Unicode alphanumeric characters (letters plus digits\n",
      "             plus underscore).\n",
      "             With LOCALE, it will match the set [0-9_] plus characters defined\n",
      "             as letters for the current locale.\n",
      "    \\W       Matches the complement of \\w.\n",
      "    \\\\       Matches a literal backslash.\n",
      "\n",
      "This module exports the following functions:\n",
      "    match     Match a regular expression pattern to the beginning of a string.\n",
      "    fullmatch Match a regular expression pattern to all of a string.\n",
      "    search    Search a string for the presence of a pattern.\n",
      "    sub       Substitute occurrences of a pattern found in a string.\n",
      "    subn      Same as sub, but also return the number of substitutions made.\n",
      "    split     Split a string by the occurrences of a pattern.\n",
      "    findall   Find all occurrences of a pattern in a string.\n",
      "    finditer  Return an iterator yielding a match object for each match.\n",
      "    compile   Compile a pattern into a RegexObject.\n",
      "    purge     Clear the regular expression cache.\n",
      "    escape    Backslash all non-alphanumerics in a string.\n",
      "\n",
      "Some of the functions in this module takes flags as optional parameters:\n",
      "    A  ASCII       For string patterns, make \\w, \\W, \\b, \\B, \\d, \\D\n",
      "                   match the corresponding ASCII character categories\n",
      "                   (rather than the whole Unicode categories, which is the\n",
      "                   default).\n",
      "                   For bytes patterns, this flag is the only available\n",
      "                   behaviour and needn't be specified.\n",
      "    I  IGNORECASE  Perform case-insensitive matching.\n",
      "    L  LOCALE      Make \\w, \\W, \\b, \\B, dependent on the current locale.\n",
      "    M  MULTILINE   \"^\" matches the beginning of lines (after a newline)\n",
      "                   as well as the string.\n",
      "                   \"$\" matches the end of lines (before a newline) as well\n",
      "                   as the end of the string.\n",
      "    S  DOTALL      \".\" matches any character at all, including the newline.\n",
      "    X  VERBOSE     Ignore whitespace and comments for nicer looking RE's.\n",
      "    U  UNICODE     For compatibility only. Ignored for string patterns (it\n",
      "                   is the default), and forbidden for bytes patterns.\n",
      "\n",
      "This module also defines an exception 'error'.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print(re.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 3), match='www'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(re.match('www', 'www.runoob.com'))  # 在起始位置匹配\n",
    "print(re.match('com', 'www.runoob.com'))         # 不在起始位置匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 3)\n"
     ]
    }
   ],
   "source": [
    "print(re.match('www', 'www.runoob.com').span())  # 在起始位置匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matchObj.group() :  Cats are smarter than dogs\n",
      "matchObj.group(1) :  Cats\n",
      "matchObj.group(2) :  smarter\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    " \n",
    "line = \"Cats are smarter than dogs\"\n",
    " \n",
    "matchObj = re.match( r'(.*) are (.*?) .*', line, re.M|re.I)\n",
    " \n",
    "if matchObj:\n",
    "    print( \"matchObj.group() : \", matchObj.group())\n",
    "    print( \"matchObj.group(1) : \", matchObj.group(1))\n",
    "    print( \"matchObj.group(2) : \", matchObj.group(2))\n",
    "else:\n",
    "    print( \"No match!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cats'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matchObj.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #正则表达式\n",
    "\n",
    "def textParse(bigString):    \n",
    "    #输入句切分形成一个单词列表\n",
    "    listOfTokens = re.split(r'\\W+', bigString) \n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2] #提取单词大于2的单词，并将字符串小写化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'wtt', 'like', 'studying']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textParse('my name is wtt, I like studying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my', 'name', 'is', 'wtt', 'I', 'like', 'studying']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r'\\W+', 'my name is wtt, I like studying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my', 'name', 'is', 'wtt', '', 'I', 'like', 'studying']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r'\\W', 'my name is wtt, I like studying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spamTest():\n",
    "    docList = []; classList = []; fullText = []\n",
    "    for i in range(1, 26):\n",
    "        wordList = textParse(open('email/spam/%d.txt' % i, encoding=\"ISO-8859-1\").read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(open('email/ham/%d.txt' % i, encoding=\"ISO-8859-1\").read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)#create vocabulary\n",
    "    trainingSet = range(50); testSet = []           #create test set\n",
    "    for i in range(10):\n",
    "        randIndex = int(np.random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(list(trainingSet)[randIndex])\n",
    "    trainMat = []; trainClasses = []\n",
    "    for docIndex in trainingSet:#train the classifier (get probs) trainNB0\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V, p1V, pSpam = trainNB0(np.array(trainMat), np.array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:        #classify the remaining items\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if classifyNB(np.array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "            print(\"classification error\", docList[docIndex])\n",
    "    print('the error rate is: ', float(errorCount)/len(testSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "docList = []\n",
    "classList = []\n",
    "fullText = []\n",
    "for i in range(1, 26):\n",
    "    wordList = textParse(open('email/spam/%d.txt' % i, encoding=\"ISO-8859-1\").read())\n",
    "    docList.append(wordList)\n",
    "    fullText.extend(wordList)\n",
    "    classList.append(1)\n",
    "    wordList = textParse(open('email/ham/%d.txt' % i, encoding=\"ISO-8859-1\").read())\n",
    "    docList.append(wordList)\n",
    "    fullText.extend(wordList)\n",
    "    classList.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docList) #25个正样本25个负样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1762"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fullText) #词汇总数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['had',\n",
       " 'there',\n",
       " 'might',\n",
       " 'betterejacu1ation',\n",
       " 'assigning',\n",
       " 'certified',\n",
       " 'have',\n",
       " 'courier',\n",
       " 'incredib1e',\n",
       " 'herbal',\n",
       " 'away',\n",
       " 'what',\n",
       " 'changes',\n",
       " 'tokyo',\n",
       " 'town',\n",
       " 'inconvenience',\n",
       " 'far',\n",
       " 'required',\n",
       " 'important',\n",
       " '492',\n",
       " 'save',\n",
       " 'development',\n",
       " 'prepared',\n",
       " 'mailing',\n",
       " 'dusty',\n",
       " 'cheers',\n",
       " 'bettererections',\n",
       " 'discussions',\n",
       " 'cs5',\n",
       " 'knocking',\n",
       " 'cats',\n",
       " '100mg',\n",
       " '156',\n",
       " 'fundamental',\n",
       " 'since',\n",
       " '396',\n",
       " 'yeah',\n",
       " 'when',\n",
       " 'butt',\n",
       " 'exhibit',\n",
       " 'sky',\n",
       " 'jewerly',\n",
       " 'cat',\n",
       " 'logged',\n",
       " 'germany',\n",
       " 'and',\n",
       " 'then',\n",
       " 'financial',\n",
       " 'strategy',\n",
       " 'try',\n",
       " 'ones',\n",
       " 'arolexbvlgari',\n",
       " 'quantitative',\n",
       " 'like',\n",
       " 'behind',\n",
       " '25mg',\n",
       " 'door',\n",
       " 'talked',\n",
       " 'bike',\n",
       " 'through',\n",
       " 'borders',\n",
       " 'plugin',\n",
       " 'experts',\n",
       " 'softwares',\n",
       " 'got',\n",
       " 'come',\n",
       " 'series',\n",
       " 'reliever',\n",
       " 'placed',\n",
       " 'message',\n",
       " 'service',\n",
       " 'modelling',\n",
       " 'release',\n",
       " 'watchesstore',\n",
       " 'parallel',\n",
       " 'python',\n",
       " 'effective',\n",
       " '430',\n",
       " 'changing',\n",
       " 'generates',\n",
       " 'per',\n",
       " 'jpgs',\n",
       " 'model',\n",
       " 'recieve',\n",
       " 'delivery',\n",
       " 'gain',\n",
       " 'ideas',\n",
       " 'wasn',\n",
       " 'who',\n",
       " '90563',\n",
       " 'regards',\n",
       " 'professional',\n",
       " 'doggy',\n",
       " 'benoit',\n",
       " 'get',\n",
       " 'windows',\n",
       " 'information',\n",
       " 'differ',\n",
       " 'interesting',\n",
       " 'shape',\n",
       " 'buyviagra',\n",
       " 'off',\n",
       " 'wholesale',\n",
       " 'hommies',\n",
       " 'office',\n",
       " 'running',\n",
       " 'blue',\n",
       " 'sounds',\n",
       " 'online',\n",
       " 'wednesday',\n",
       " 'carlo',\n",
       " 'listed',\n",
       " 'functionalities',\n",
       " 'phentermin',\n",
       " 'least',\n",
       " 'holiday',\n",
       " 'china',\n",
       " '50092',\n",
       " 'chapter',\n",
       " 'way',\n",
       " 'below',\n",
       " '14th',\n",
       " 'museum',\n",
       " 'google',\n",
       " '0nline',\n",
       " 'brand',\n",
       " 'heard',\n",
       " 'view',\n",
       " 'find',\n",
       " 'done',\n",
       " 'riding',\n",
       " 'hamm',\n",
       " 'call',\n",
       " 'want',\n",
       " 'concise',\n",
       " 'pills',\n",
       " '325',\n",
       " 'analgesic',\n",
       " 'wilmott',\n",
       " 'was',\n",
       " 'foaming',\n",
       " 'place',\n",
       " 'has',\n",
       " 'sure',\n",
       " 'brandviagra',\n",
       " 'good',\n",
       " 'bathroom',\n",
       " 'faster',\n",
       " 'insights',\n",
       " 'issues',\n",
       " '200',\n",
       " 'drunk',\n",
       " 'website',\n",
       " 'reply',\n",
       " 'mathematician',\n",
       " 'must',\n",
       " 'endorsed',\n",
       " 'color',\n",
       " 'fast',\n",
       " 'freeviagra',\n",
       " '86152',\n",
       " 'new',\n",
       " 'favorite',\n",
       " 'saw',\n",
       " 'advocate',\n",
       " 'nature',\n",
       " 'opportunity',\n",
       " 'contact',\n",
       " 'but',\n",
       " 'naturalpenisenhancement',\n",
       " 'over',\n",
       " 'hello',\n",
       " 'connection',\n",
       " 'accept',\n",
       " 'may',\n",
       " 'pharmacy',\n",
       " 'gains',\n",
       " 'launch',\n",
       " 'located',\n",
       " 'bad',\n",
       " 'discount',\n",
       " 'went',\n",
       " 'finance',\n",
       " 'mail',\n",
       " 'number',\n",
       " 'pages',\n",
       " 'attaching',\n",
       " 'thanks',\n",
       " 'home',\n",
       " '385',\n",
       " '2010',\n",
       " 'see',\n",
       " 'photoshop',\n",
       " 'team',\n",
       " 'adobe',\n",
       " 'them',\n",
       " '300x',\n",
       " 'pictures',\n",
       " '750',\n",
       " 'couple',\n",
       " 'each',\n",
       " 'jocelyn',\n",
       " 'microsoft',\n",
       " 'extended',\n",
       " 'glimpse',\n",
       " 'tiffany',\n",
       " 'prices',\n",
       " 'trusted',\n",
       " 'invitation',\n",
       " 'volume',\n",
       " '199',\n",
       " 'need',\n",
       " 'hotel',\n",
       " 'fine',\n",
       " 'tickets',\n",
       " 'pain',\n",
       " 'network',\n",
       " 'of_penisen1argement',\n",
       " 'amex',\n",
       " 'approach',\n",
       " 'example',\n",
       " 'wallets',\n",
       " 'competitive',\n",
       " 'great',\n",
       " 'starting',\n",
       " 'said',\n",
       " 'not',\n",
       " 'perhaps',\n",
       " 'kerry',\n",
       " 'fractal',\n",
       " 'reputable',\n",
       " 'ems',\n",
       " 'shipping',\n",
       " 'inches',\n",
       " 'book',\n",
       " 'lunch',\n",
       " 'been',\n",
       " 'create',\n",
       " 'both',\n",
       " 'received',\n",
       " 'approved',\n",
       " 'pavilion',\n",
       " 'brands',\n",
       " 'job',\n",
       " 'forum',\n",
       " '588',\n",
       " 'announcement',\n",
       " 'suggest',\n",
       " 'doctor',\n",
       " 'thing',\n",
       " 'acrobat',\n",
       " '129',\n",
       " 'pretty',\n",
       " 'will',\n",
       " 'another',\n",
       " 'prototype',\n",
       " 'moderately',\n",
       " 'edit',\n",
       " 'others',\n",
       " 'possible',\n",
       " 'ready',\n",
       " 'answer',\n",
       " 'access',\n",
       " 'once',\n",
       " 'commented',\n",
       " 'drugs',\n",
       " 'name',\n",
       " 'dozen',\n",
       " 'pill',\n",
       " 'guy',\n",
       " 'expo',\n",
       " 'cartier',\n",
       " 'chinese',\n",
       " 'hangzhou',\n",
       " 'retirement',\n",
       " 'worldwide',\n",
       " 'sliding',\n",
       " 'ofejacu1ate',\n",
       " '15mg',\n",
       " 'than',\n",
       " 'superb',\n",
       " 'percocet',\n",
       " 'definitely',\n",
       " 'sent',\n",
       " 'serial',\n",
       " 'tent',\n",
       " 'window',\n",
       " 'too',\n",
       " 'hermes',\n",
       " '119',\n",
       " 'province',\n",
       " 'working',\n",
       " 'amazing',\n",
       " 'writing',\n",
       " 'magazine',\n",
       " 'could',\n",
       " 'famous',\n",
       " 'status',\n",
       " 'explosive',\n",
       " 'derivatives',\n",
       " 'programming',\n",
       " 'aged',\n",
       " 'top',\n",
       " 'fermi',\n",
       " 'visa',\n",
       " 'specifically',\n",
       " 'out',\n",
       " 'tesla',\n",
       " 'because',\n",
       " 'enough',\n",
       " 'add',\n",
       " 'lined',\n",
       " '292',\n",
       " '291',\n",
       " 'most',\n",
       " 'fans',\n",
       " 'stepp',\n",
       " '5mg',\n",
       " '130',\n",
       " 'horn',\n",
       " 'focusing',\n",
       " 'they',\n",
       " 'control',\n",
       " 'safe',\n",
       " 'hold',\n",
       " 'rude',\n",
       " 'inform',\n",
       " 'groups',\n",
       " 'vivek',\n",
       " 'oem',\n",
       " 'significantly',\n",
       " 'harderecetions',\n",
       " 'gas',\n",
       " 'mandarin',\n",
       " 'zolpidem',\n",
       " 'tour',\n",
       " 'thickness',\n",
       " 'cold',\n",
       " 'focus',\n",
       " 'jay',\n",
       " 'also',\n",
       " 'pro',\n",
       " 'earn',\n",
       " '1924',\n",
       " 'mandelbrot',\n",
       " 'just',\n",
       " 'cheap',\n",
       " 'now',\n",
       " '174623',\n",
       " 'docs',\n",
       " 'gucci',\n",
       " 'february',\n",
       " 'either',\n",
       " 'night',\n",
       " 'came',\n",
       " 'class',\n",
       " 'york',\n",
       " 'credit',\n",
       " 'thirumalai',\n",
       " 'item',\n",
       " 'safest',\n",
       " '138',\n",
       " 'warranty',\n",
       " '322',\n",
       " 'use',\n",
       " 'for',\n",
       " 'members',\n",
       " 'jquery',\n",
       " 'spaying',\n",
       " 'automatically',\n",
       " 'withoutprescription',\n",
       " 'reservation',\n",
       " 'bags',\n",
       " 'site',\n",
       " 'told',\n",
       " 'winter',\n",
       " 'does',\n",
       " 'experience',\n",
       " 'support',\n",
       " 'length',\n",
       " '10mg',\n",
       " 'keep',\n",
       " 'discreet',\n",
       " 'page',\n",
       " 'ups',\n",
       " 'hydrocodone',\n",
       " 'school',\n",
       " 'cca',\n",
       " 'location',\n",
       " 'car',\n",
       " 'supporting',\n",
       " 'phone',\n",
       " 'items',\n",
       " 'wilson',\n",
       " 'runs',\n",
       " 'girl',\n",
       " 'download',\n",
       " '120',\n",
       " 'let',\n",
       " 'his',\n",
       " 'creative',\n",
       " 'design',\n",
       " 'jqplot',\n",
       " 'don',\n",
       " 'know',\n",
       " 'party',\n",
       " 'dior',\n",
       " 'with',\n",
       " 'jar',\n",
       " 'yesterday',\n",
       " 'rock',\n",
       " 'accepted',\n",
       " 'past',\n",
       " 'storage',\n",
       " 'same',\n",
       " 'wrote',\n",
       " 'thousand',\n",
       " 'died',\n",
       " 'ultimate',\n",
       " 'designed',\n",
       " 'incoming',\n",
       " 'hours',\n",
       " 'two',\n",
       " 'mba',\n",
       " 'art',\n",
       " 'all',\n",
       " 'please',\n",
       " 'lists',\n",
       " 'viagranoprescription',\n",
       " 'mandatory',\n",
       " 'hotels',\n",
       " 'using',\n",
       " 'computing',\n",
       " 'moneyback',\n",
       " 'assistance',\n",
       " 'cannot',\n",
       " 'expertise',\n",
       " 'natural',\n",
       " 'income',\n",
       " 'sites',\n",
       " 'articles',\n",
       " 'www',\n",
       " 'noprescription',\n",
       " 'zach',\n",
       " '195',\n",
       " 'such',\n",
       " 'october',\n",
       " 'includes',\n",
       " 'ma1eenhancement',\n",
       " 'methylmorphine',\n",
       " 'opioid',\n",
       " 'com',\n",
       " 'generation',\n",
       " 'instead',\n",
       " 'shipment',\n",
       " 'millions',\n",
       " 'stuff',\n",
       " '30mg',\n",
       " 'price',\n",
       " 'notification',\n",
       " 'only',\n",
       " 'features',\n",
       " 'often',\n",
       " 'enabled',\n",
       " 'tool',\n",
       " 'made',\n",
       " 'store',\n",
       " 'fbi',\n",
       " 'plane',\n",
       " 'severepain',\n",
       " 'care',\n",
       " 'capabilities',\n",
       " 'dhl',\n",
       " 'fda',\n",
       " 'major',\n",
       " 'link',\n",
       " 'trip',\n",
       " 'cuda',\n",
       " 'narcotic',\n",
       " 'full',\n",
       " 'comment',\n",
       " 'address',\n",
       " 'help',\n",
       " 'web',\n",
       " 'chance',\n",
       " 'time',\n",
       " 'vicodin',\n",
       " 'about',\n",
       " 'think',\n",
       " '100',\n",
       " 'customized',\n",
       " 'bargains',\n",
       " 'encourage',\n",
       " 'ambiem',\n",
       " 'sorry',\n",
       " 'uses',\n",
       " 'ferguson',\n",
       " 'products',\n",
       " 'treat',\n",
       " 'today',\n",
       " 'here',\n",
       " 'scenic',\n",
       " 'copy',\n",
       " 'thailand',\n",
       " 'any',\n",
       " 'permanantly',\n",
       " 'coast',\n",
       " 'success',\n",
       " 'huge',\n",
       " 'john',\n",
       " 'check',\n",
       " 'strategic',\n",
       " 'those',\n",
       " '180',\n",
       " '366',\n",
       " 'peter',\n",
       " 'would',\n",
       " 'station',\n",
       " 'much',\n",
       " 'brained',\n",
       " 'automatic',\n",
       " '513',\n",
       " 'bin',\n",
       " 'individual',\n",
       " 'nvidia',\n",
       " 'used',\n",
       " 'enjoy',\n",
       " 'source',\n",
       " 'having',\n",
       " 'thought',\n",
       " 'doors',\n",
       " '50mg',\n",
       " 'money',\n",
       " 'these',\n",
       " 'plus',\n",
       " 'turd',\n",
       " 'days',\n",
       " 'computer',\n",
       " 'whybrew',\n",
       " 'ryan',\n",
       " 'train',\n",
       " '66343',\n",
       " 'femaleviagra',\n",
       " 'survive',\n",
       " 'free',\n",
       " 'work',\n",
       " 'father',\n",
       " 'note',\n",
       " 'everything',\n",
       " 'held',\n",
       " 'pick',\n",
       " 'this',\n",
       " 'being',\n",
       " '2011',\n",
       " 'food',\n",
       " 'guaranteeed',\n",
       " 'thread',\n",
       " 'supplement',\n",
       " 'canadian',\n",
       " 'methods',\n",
       " 'based',\n",
       " 'sophisticated',\n",
       " 'buy',\n",
       " 'follow',\n",
       " 'latest',\n",
       " 'inspired',\n",
       " 'yay',\n",
       " 'needed',\n",
       " 'hope',\n",
       " '203',\n",
       " 'jose',\n",
       " 'style',\n",
       " 'rent',\n",
       " 'forward',\n",
       " 'julius',\n",
       " 'decision',\n",
       " 'files',\n",
       " 'featured',\n",
       " 'improving',\n",
       " 'tabs',\n",
       " 'core',\n",
       " 'going',\n",
       " 'cards',\n",
       " 'that',\n",
       " '100m',\n",
       " 'welcome',\n",
       " 'works',\n",
       " 'where',\n",
       " 'can',\n",
       " 'creation',\n",
       " 'code',\n",
       " 'things',\n",
       " 'eugene',\n",
       " '225',\n",
       " 'grow',\n",
       " 'cost',\n",
       " 'order',\n",
       " 'questions',\n",
       " 'more',\n",
       " 'while',\n",
       " 'right',\n",
       " 'scifinance',\n",
       " 'intenseorgasns',\n",
       " '219',\n",
       " 'level',\n",
       " 'gpu',\n",
       " 'upload',\n",
       " 'roofer',\n",
       " 'net',\n",
       " '562',\n",
       " 'doing',\n",
       " 'troy',\n",
       " 'yourpenis',\n",
       " 'email',\n",
       " 'low',\n",
       " 'storedetailview_98',\n",
       " 'moderate',\n",
       " 'mathematics',\n",
       " 'from',\n",
       " 'codeine',\n",
       " 'easily',\n",
       " 'ordercializviagra',\n",
       " 'well',\n",
       " 'high',\n",
       " 'are',\n",
       " 'life',\n",
       " 'express',\n",
       " 'take',\n",
       " 'requested',\n",
       " 'finder',\n",
       " 'update',\n",
       " 'meet',\n",
       " 'inside',\n",
       " 'day',\n",
       " '625',\n",
       " 'selected',\n",
       " 'increase',\n",
       " 'haloney',\n",
       " 'linkedin',\n",
       " 'watches',\n",
       " 'close',\n",
       " 'specifications',\n",
       " 'looking',\n",
       " 'leaves',\n",
       " 'knew',\n",
       " 'longer',\n",
       " 'share',\n",
       " 'grounds',\n",
       " 'pricing',\n",
       " 'mom',\n",
       " 'program',\n",
       " 'rain',\n",
       " 'louis',\n",
       " 'the',\n",
       " 'titles',\n",
       " 'your',\n",
       " 'owner',\n",
       " 'management',\n",
       " 'monte',\n",
       " 'biggerpenis',\n",
       " 'genuine',\n",
       " 'game',\n",
       " '2007',\n",
       " 'back',\n",
       " 'vuitton',\n",
       " 'pls',\n",
       " 'risk',\n",
       " 'arvind',\n",
       " 'speedpost',\n",
       " 'watson',\n",
       " 'some',\n",
       " 'should',\n",
       " 'via',\n",
       " 'year',\n",
       " 'giants',\n",
       " 'you',\n",
       " 'one',\n",
       " 'oris',\n",
       " 'fedex',\n",
       " '570',\n",
       " 'file',\n",
       " 'learn',\n",
       " 'business',\n",
       " 'proven',\n",
       " 'signed',\n",
       " 'private',\n",
       " 'transformed',\n",
       " 'http',\n",
       " 'quality',\n",
       " 'how',\n",
       " 'thank',\n",
       " 'group']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabList = createVocabList(docList)#创建词汇表\n",
    "vocabList "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "692"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSet = range(50)\n",
    "\n",
    "#随机选择10份作为测试集\n",
    "testSet = []           #create test set\n",
    "for i in range(10):\n",
    "    randIndex = int(np.random.uniform(0, len(trainingSet))) #功能：从一个均匀分布[low,high)中随机采样，注意定义域是左闭右开，即包含low，不包含high.\n",
    "    testSet.append(trainingSet[randIndex]) #在测试集中加入对应随机的index的样本数据\n",
    "    trainingSet = list(trainingSet)\n",
    "    del(trainingSet[randIndex]) #并在训练集中删除对应提取到测试集中的数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 16,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22, 17, 49, 31, 15, 10, 37, 41, 32, 26]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainMat = []\n",
    "trainClasses = []\n",
    "for docIndex in trainingSet:#遍历训练集的样本索引\n",
    "    trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex])) #将样本词汇进行词向量转换并保存训练集输入矩阵当中\n",
    "    trainClasses.append(classList[docIndex]) #样本对应的类别标签\n",
    "p0V, p1V, pSpam = trainNB0(np.array(trainMat), np.array(trainClasses)) #生成训练集生成的朴素贝叶斯分类模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-5.56068163, -5.04985601, -6.65929392, -6.65929392, -5.96614674,\n",
       "        -6.65929392, -4.86753445, -6.65929392, -6.65929392, -6.65929392,\n",
       "        -5.96614674, -5.56068163, -5.96614674, -5.96614674, -5.96614674,\n",
       "        -5.96614674, -5.96614674, -5.96614674, -5.96614674, -6.65929392,\n",
       "        -6.65929392, -5.96614674, -5.96614674, -5.96614674, -5.96614674,\n",
       "        -5.96614674, -6.65929392, -5.96614674, -6.65929392, -6.65929392,\n",
       "        -5.96614674, -6.65929392, -6.65929392, -5.96614674, -5.96614674,\n",
       "        -6.65929392, -6.65929392, -5.96614674, -5.96614674, -6.65929392,\n",
       "        -6.65929392, -6.65929392, -5.96614674, -5.96614674, -6.65929392,\n",
       "        -3.44041809, -5.96614674, -6.65929392, -5.96614674, -5.96614674,\n",
       "        -5.96614674, -6.65929392, -5.96614674, -5.27299956, -5.96614674,\n",
       "        -6.65929392, -5.96614674, -5.56068163, -5.96614674, -5.96614674,\n",
       "        -6.65929392, -5.96614674, -6.65929392, -6.65929392, -5.56068163,\n",
       "        -5.04985601, -5.96614674, -6.65929392, -6.65929392, -5.96614674,\n",
       "        -5.96614674, -5.96614674, -5.96614674, -6.65929392, -5.96614674,\n",
       "        -5.96614674, -6.65929392, -6.65929392, -6.65929392, -5.56068163,\n",
       "        -6.65929392, -5.96614674, -5.04985601, -6.65929392, -6.65929392,\n",
       "        -6.65929392, -5.56068163, -6.65929392, -5.96614674, -5.96614674,\n",
       "        -5.96614674, -5.96614674, -5.96614674, -5.27299956, -5.04985601,\n",
       "        -5.96614674, -6.65929392, -6.65929392, -5.96614674, -5.96614674,\n",
       "        -6.65929392, -6.65929392, -6.65929392, -5.96614674, -6.65929392,\n",
       "        -5.96614674, -6.65929392, -5.96614674, -6.65929392, -5.96614674,\n",
       "        -5.96614674, -6.65929392, -5.96614674, -6.65929392, -5.96614674,\n",
       "        -5.96614674, -6.65929392, -5.96614674, -5.96614674, -5.96614674,\n",
       "        -5.96614674, -5.96614674, -5.96614674, -4.09434456, -6.65929392,\n",
       "        -6.65929392, -5.96614674, -5.96614674, -6.65929392, -5.56068163,\n",
       "        -5.96614674, -5.96614674, -5.96614674, -5.96614674, -5.96614674,\n",
       "        -6.65929392, -6.65929392, -6.65929392, -5.27299956, -4.86753445,\n",
       "        -5.96614674, -5.96614674, -5.96614674, -5.96614674, -6.65929392,\n",
       "        -5.04985601, -5.96614674, -5.96614674, -5.96614674, -5.96614674,\n",
       "        -6.65929392, -5.96614674, -5.96614674, -5.56068163, -5.96614674,\n",
       "        -5.96614674, -6.65929392, -6.65929392, -6.65929392, -6.65929392,\n",
       "        -5.96614674, -5.04985601, -5.96614674, -6.65929392, -5.96614674,\n",
       "        -5.56068163, -6.65929392, -6.65929392, -6.65929392, -6.65929392,\n",
       "        -6.65929392, -5.96614674, -5.96614674, -6.65929392, -6.65929392,\n",
       "        -6.65929392, -6.65929392, -5.96614674, -6.65929392, -6.65929392,\n",
       "        -6.65929392, -5.56068163, -5.96614674, -6.65929392, -6.65929392,\n",
       "        -5.27299956, -5.96614674, -5.96614674, -6.65929392, -6.65929392,\n",
       "        -5.27299956, -5.27299956, -6.65929392, -5.96614674, -6.65929392,\n",
       "        -5.56068163, -5.96614674, -5.56068163, -6.65929392, -5.96614674,\n",
       "        -6.65929392, -5.96614674, -6.65929392, -5.96614674, -6.65929392,\n",
       "        -6.65929392, -6.65929392, -6.65929392, -6.65929392, -6.65929392,\n",
       "        -6.65929392, -6.65929392, -5.96614674, -5.96614674, -6.65929392,\n",
       "        -6.65929392, -5.96614674, -6.65929392, -6.65929392, -5.96614674,\n",
       "        -5.96614674, -6.65929392, -6.65929392, -6.65929392, -5.96614674,\n",
       "        -5.96614674, -5.27299956, -5.96614674, -5.56068163, -5.96614674,\n",
       "        -6.65929392, -6.65929392, -6.65929392, -6.65929392, -5.56068163,\n",
       "        -5.96614674, -5.56068163, -5.96614674, -5.96614674, -5.96614674,\n",
       "        -6.65929392, -6.65929392, -6.65929392, -5.96614674, -5.96614674,\n",
       "        -6.65929392, -5.96614674, -5.96614674, -6.65929392, -5.96614674,\n",
       "        -6.65929392, -6.65929392, -6.65929392, -4.57985238, -5.96614674,\n",
       "        -5.96614674, -6.65929392, -5.96614674, -5.96614674, -5.96614674,\n",
       "        -6.65929392, -4.26139865, -5.56068163, -5.96614674, -5.04985601,\n",
       "        -6.65929392, -6.65929392, -5.96614674, -6.65929392, -5.96614674,\n",
       "        -6.65929392, -6.65929392, -5.96614674, -6.65929392, -6.65929392,\n",
       "        -6.65929392, -5.96614674, -6.65929392, -6.65929392, -5.96614674,\n",
       "        -6.65929392, -6.65929392, -5.96614674, -6.65929392, -5.96614674,\n",
       "        -5.96614674, -5.56068163, -5.96614674, -6.65929392, -6.65929392,\n",
       "        -6.65929392, -5.56068163, -6.65929392, -5.96614674, -5.56068163,\n",
       "        -5.27299956, -6.65929392, -4.71338377, -6.65929392, -5.96614674,\n",
       "        -5.96614674, -5.96614674, -5.96614674, -5.96614674, -6.65929392,\n",
       "        -5.96614674, -5.56068163, -5.96614674, -6.65929392, -5.96614674,\n",
       "        -5.56068163, -5.96614674, -6.65929392, -6.65929392, -6.65929392,\n",
       "        -5.96614674, -5.96614674, -6.65929392, -6.65929392, -5.96614674,\n",
       "        -5.96614674, -5.04985601, -6.65929392, -6.65929392, -6.65929392,\n",
       "        -6.65929392, -5.96614674, -5.04985601, -5.96614674, -6.65929392,\n",
       "        -5.96614674, -6.65929392, -5.96614674, -5.96614674, -6.65929392,\n",
       "        -5.96614674, -6.65929392, -5.96614674, -5.96614674, -5.56068163,\n",
       "        -5.96614674, -6.65929392, -6.65929392, -5.56068163, -5.04985601,\n",
       "        -5.56068163, -6.65929392, -5.27299956, -5.96614674, -4.86753445,\n",
       "        -6.65929392, -5.96614674, -5.56068163, -5.96614674, -5.96614674,\n",
       "        -5.96614674, -5.96614674, -6.65929392, -5.96614674, -6.65929392,\n",
       "        -6.65929392, -6.65929392, -6.65929392, -6.65929392, -5.96614674,\n",
       "        -4.46206934, -5.56068163, -5.96614674, -5.96614674, -5.56068163,\n",
       "        -6.65929392, -6.65929392, -6.65929392, -5.27299956, -5.96614674,\n",
       "        -5.96614674, -5.96614674, -6.65929392, -4.86753445, -6.65929392,\n",
       "        -6.65929392, -5.96614674, -6.65929392, -5.96614674, -6.65929392,\n",
       "        -6.65929392, -5.96614674, -5.96614674, -6.65929392, -5.96614674,\n",
       "        -5.96614674, -5.96614674, -6.65929392, -6.65929392, -5.96614674,\n",
       "        -6.65929392, -5.56068163, -6.65929392, -5.56068163, -5.56068163,\n",
       "        -5.96614674, -5.96614674, -5.56068163, -5.96614674, -5.04985601,\n",
       "        -6.65929392, -6.65929392, -4.46206934, -6.65929392, -5.96614674,\n",
       "        -6.65929392, -6.65929392, -5.96614674, -5.96614674, -5.96614674,\n",
       "        -5.04985601, -6.65929392, -5.96614674, -6.65929392, -5.96614674,\n",
       "        -6.65929392, -6.65929392, -5.96614674, -5.96614674, -5.96614674,\n",
       "        -5.27299956, -6.65929392, -5.96614674, -6.65929392, -5.96614674,\n",
       "        -5.96614674, -5.56068163, -5.96614674, -6.65929392, -6.65929392,\n",
       "        -6.65929392, -5.96614674, -6.65929392, -6.65929392, -5.04985601,\n",
       "        -5.96614674, -5.56068163, -6.65929392, -5.56068163, -6.65929392,\n",
       "        -5.96614674, -5.96614674, -5.96614674, -6.65929392, -6.65929392,\n",
       "        -6.65929392, -4.86753445, -5.96614674, -5.96614674, -6.65929392,\n",
       "        -6.65929392, -5.56068163, -6.65929392, -6.65929392, -6.65929392,\n",
       "        -5.96614674, -5.56068163, -5.96614674, -5.27299956, -5.96614674,\n",
       "        -5.96614674, -5.96614674, -5.96614674, -6.65929392, -6.65929392,\n",
       "        -5.96614674, -5.96614674, -6.65929392, -6.65929392, -6.65929392,\n",
       "        -5.96614674, -5.96614674, -5.96614674, -6.65929392, -6.65929392,\n",
       "        -5.27299956, -6.65929392, -5.96614674, -6.65929392, -6.65929392,\n",
       "        -5.27299956, -6.65929392, -5.04985601, -5.56068163, -6.65929392,\n",
       "        -5.96614674, -6.65929392, -5.96614674, -6.65929392, -5.96614674,\n",
       "        -5.96614674, -5.96614674, -5.96614674, -6.65929392, -5.56068163,\n",
       "        -6.65929392, -5.96614674, -5.96614674, -5.96614674, -5.96614674,\n",
       "        -6.65929392, -6.65929392, -6.65929392, -6.65929392, -5.96614674,\n",
       "        -5.96614674, -5.96614674, -5.96614674, -6.65929392, -6.65929392,\n",
       "        -4.86753445, -5.56068163, -5.96614674, -5.96614674, -5.96614674,\n",
       "        -5.96614674, -6.65929392, -4.86753445, -5.96614674, -5.96614674,\n",
       "        -5.96614674, -6.65929392, -5.27299956, -5.96614674, -5.96614674,\n",
       "        -5.96614674, -6.65929392, -6.65929392, -5.27299956, -6.65929392,\n",
       "        -5.96614674, -6.65929392, -5.96614674, -5.96614674, -5.56068163,\n",
       "        -5.96614674, -5.96614674, -6.65929392, -5.96614674, -5.96614674,\n",
       "        -5.56068163, -5.96614674, -6.65929392, -6.65929392, -6.65929392,\n",
       "        -5.96614674, -4.17438727, -5.56068163, -5.96614674, -5.96614674,\n",
       "        -6.65929392, -5.96614674, -6.65929392, -6.65929392, -6.65929392,\n",
       "        -6.65929392, -5.96614674, -6.65929392, -5.96614674, -5.56068163,\n",
       "        -5.96614674, -5.96614674, -6.65929392, -5.96614674, -6.65929392,\n",
       "        -5.96614674, -5.96614674, -5.96614674, -6.65929392, -6.65929392,\n",
       "        -5.96614674, -4.71338377, -5.96614674, -5.96614674, -6.65929392,\n",
       "        -5.96614674, -5.27299956, -6.65929392, -4.09434456, -5.96614674,\n",
       "        -5.96614674, -6.65929392, -5.96614674, -4.35670883, -5.96614674,\n",
       "        -5.04985601, -5.96614674, -5.96614674, -6.65929392, -6.65929392,\n",
       "        -6.65929392, -6.65929392, -6.65929392, -5.27299956, -5.96614674,\n",
       "        -5.56068163, -5.27299956, -6.65929392, -6.65929392, -5.96614674,\n",
       "        -5.04985601, -5.56068163, -5.96614674, -6.65929392, -6.65929392,\n",
       "        -5.56068163, -5.96614674, -6.65929392, -5.04985601, -6.65929392,\n",
       "        -6.65929392, -6.65929392, -5.96614674, -4.71338377, -6.65929392,\n",
       "        -5.96614674, -6.65929392, -5.96614674, -5.96614674, -4.46206934,\n",
       "        -6.65929392, -6.65929392, -5.56068163, -5.96614674, -6.65929392,\n",
       "        -5.96614674, -5.96614674, -6.65929392, -6.65929392, -6.65929392,\n",
       "        -6.65929392, -6.65929392, -5.56068163, -5.27299956, -6.65929392,\n",
       "        -5.56068163, -5.96614674, -5.96614674, -6.65929392, -5.96614674,\n",
       "        -5.96614674, -5.56068163, -5.96614674, -5.27299956, -5.96614674,\n",
       "        -5.96614674, -5.96614674, -6.65929392, -3.10394586, -6.65929392,\n",
       "        -4.26139865, -5.96614674, -5.96614674, -5.96614674, -6.65929392,\n",
       "        -6.65929392, -5.96614674, -6.65929392, -6.65929392, -6.65929392,\n",
       "        -5.96614674, -5.56068163, -5.56068163, -6.65929392, -6.65929392,\n",
       "        -5.27299956, -5.96614674, -6.65929392, -5.96614674, -5.56068163,\n",
       "        -3.29199809, -5.27299956, -6.65929392, -6.65929392, -6.65929392,\n",
       "        -5.96614674, -6.65929392, -6.65929392, -6.65929392, -5.96614674,\n",
       "        -6.65929392, -6.65929392, -4.86753445, -6.65929392, -5.56068163,\n",
       "        -5.96614674, -5.04985601]),\n",
       " array([-6.5366916 , -6.5366916 , -6.5366916 , -4.45725006, -6.5366916 ,\n",
       "        -6.5366916 , -4.2341065 , -5.15039724, -4.45725006, -4.45725006,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -5.15039724, -6.5366916 , -5.84354442,\n",
       "        -4.59078145, -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -5.84354442, -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -5.84354442, -6.5366916 , -6.5366916 ,\n",
       "        -5.84354442, -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -5.15039724, -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -3.70347825, -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -5.15039724, -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -5.43807931, -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -5.15039724, -6.5366916 ,\n",
       "        -6.5366916 , -5.43807931, -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -5.15039724, -5.84354442,\n",
       "        -4.45725006, -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -5.15039724,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -4.92725369, -5.43807931, -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -4.45725006, -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -4.92725369, -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -5.43807931,\n",
       "        -5.15039724, -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -4.74493213, -5.84354442, -5.43807931, -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -5.84354442, -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -4.45725006, -6.5366916 , -6.5366916 , -5.15039724,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -5.43807931,\n",
       "        -5.84354442, -6.5366916 , -6.5366916 , -5.43807931, -6.5366916 ,\n",
       "        -5.43807931, -4.45725006, -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -5.15039724, -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -5.43807931,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -5.43807931, -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -5.15039724, -5.43807931, -5.84354442, -6.5366916 , -4.45725006,\n",
       "        -5.84354442, -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -5.43807931, -6.5366916 , -5.84354442, -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -5.15039724, -5.84354442, -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -5.15039724, -5.15039724, -5.43807931, -4.33946702, -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -5.43807931, -6.5366916 , -4.59078145, -6.5366916 , -6.5366916 ,\n",
       "        -5.84354442, -6.5366916 , -6.5366916 , -4.45725006, -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -5.15039724, -6.5366916 ,\n",
       "        -6.5366916 , -5.43807931, -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -4.59078145, -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -5.15039724, -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -4.45725006, -4.13879632, -6.5366916 ,\n",
       "        -5.43807931, -4.59078145, -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -5.15039724, -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -4.45725006, -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -5.15039724, -6.5366916 , -4.45725006, -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -5.43807931, -6.5366916 , -4.59078145,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -5.43807931, -5.84354442, -5.43807931,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -5.43807931, -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -4.45725006, -4.2341065 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -4.45725006, -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -4.45725006, -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -5.43807931, -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -5.15039724, -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -5.15039724, -6.5366916 , -6.5366916 ,\n",
       "        -5.84354442, -6.5366916 , -5.15039724, -6.5366916 , -6.5366916 ,\n",
       "        -3.97174224, -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -5.43807931, -6.5366916 , -4.59078145, -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -4.33946702, -6.5366916 , -4.45725006,\n",
       "        -6.5366916 , -6.5366916 , -5.43807931, -6.5366916 , -5.15039724,\n",
       "        -5.84354442, -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -4.92725369, -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -4.59078145, -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -5.15039724, -5.43807931, -6.5366916 , -6.5366916 ,\n",
       "        -4.45725006, -5.43807931, -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -4.45725006,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -4.74493213, -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -5.43807931, -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -4.45725006, -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -5.15039724, -6.5366916 , -5.43807931,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -5.43807931, -5.43807931,\n",
       "        -5.43807931, -6.5366916 , -6.5366916 , -6.5366916 , -5.15039724,\n",
       "        -5.84354442, -6.5366916 , -4.59078145, -5.84354442, -6.5366916 ,\n",
       "        -4.59078145, -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -5.43807931,\n",
       "        -6.5366916 , -6.5366916 , -5.15039724, -5.43807931, -5.15039724,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -4.92725369, -5.15039724,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -5.84354442, -5.43807931, -6.5366916 , -6.5366916 , -3.16939577,\n",
       "        -6.5366916 , -5.43807931, -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -5.84354442, -5.43807931, -5.15039724,\n",
       "        -5.43807931, -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -4.45725006, -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -5.84354442, -6.5366916 , -6.5366916 , -5.43807931, -5.43807931,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -5.43807931, -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -5.43807931, -5.15039724, -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -5.84354442, -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -5.84354442, -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -5.84354442,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -4.45725006, -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -5.43807931, -6.5366916 , -5.84354442, -5.43807931, -5.84354442,\n",
       "        -6.5366916 , -6.5366916 , -4.2341065 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -4.92725369,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -5.43807931,\n",
       "        -6.5366916 , -6.5366916 , -5.15039724, -5.43807931, -6.5366916 ,\n",
       "        -6.5366916 , -5.43807931, -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -5.43807931, -5.84354442,\n",
       "        -5.43807931, -4.74493213, -6.5366916 , -4.92725369, -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -4.45725006, -5.43807931, -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -5.84354442, -5.43807931,\n",
       "        -6.5366916 , -6.5366916 , -3.8286414 , -6.5366916 , -5.43807931,\n",
       "        -6.5366916 , -5.43807931, -6.5366916 , -5.84354442, -4.2341065 ,\n",
       "        -6.5366916 , -5.43807931, -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -5.15039724, -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -5.43807931,\n",
       "        -6.5366916 , -3.8286414 , -6.5366916 , -6.5366916 , -4.45725006,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 , -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 , -5.15039724, -4.92725369, -6.5366916 ,\n",
       "        -4.92725369, -6.5366916 , -6.5366916 , -6.5366916 , -5.84354442,\n",
       "        -5.43807931, -6.5366916 , -6.5366916 , -6.5366916 , -5.15039724,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -5.15039724, -5.15039724,\n",
       "        -6.5366916 , -6.5366916 , -5.15039724, -5.15039724, -6.5366916 ,\n",
       "        -4.13879632, -6.5366916 , -5.15039724, -4.92725369, -5.43807931,\n",
       "        -6.5366916 , -6.5366916 , -6.5366916 , -5.43807931, -6.5366916 ,\n",
       "        -5.43807931, -6.5366916 , -6.5366916 , -4.74493213, -6.5366916 ,\n",
       "        -6.5366916 , -6.5366916 ]),\n",
       " 0.525)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0V, p1V, pSpam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification error ['yeah', 'ready', 'may', 'not', 'here', 'because', 'jar', 'jar', 'has', 'plane', 'tickets', 'germany', 'for']\n",
      "classification error ['oem', 'adobe', 'microsoft', 'softwares', 'fast', 'order', 'and', 'download', 'microsoft', 'office', 'professional', 'plus', '2007', '2010', '129', 'microsoft', 'windows', 'ultimate', '119', 'adobe', 'photoshop', 'cs5', 'extended', 'adobe', 'acrobat', 'pro', 'extended', 'windows', 'professional', 'thousand', 'more', 'titles']\n",
      "classification error ['home', 'based', 'business', 'opportunity', 'knocking', 'your', 'door', 'don', 'rude', 'and', 'let', 'this', 'chance', 'you', 'can', 'earn', 'great', 'income', 'and', 'find', 'your', 'financial', 'life', 'transformed', 'learn', 'more', 'here', 'your', 'success', 'work', 'from', 'home', 'finder', 'experts']\n",
      "the error rate is:  0.3\n"
     ]
    }
   ],
   "source": [
    "errorCount = 0\n",
    "for docIndex in testSet:        #测试集样本索引\n",
    "    wordVector = bagOfWords2VecMN(vocabList, docList[docIndex]) #词向量转化\n",
    "    if classifyNB(np.array(wordVector), p0V, p1V, pSpam) != classList[docIndex]: #分类测试结果是否与样本标签相同\n",
    "        \n",
    "        errorCount += 1\n",
    "        print(\"classification error\", docList[docIndex])\n",
    "print('the error rate is: ', float(errorCount)/len(testSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
